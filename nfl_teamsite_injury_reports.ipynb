{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6beef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_team_urls = [\n",
    "    # AFC East\n",
    "    \"https://www.buffalobills.com/\", \"https://www.miamidolphins.com/\", \"https://www.patriots.com/\", \"https://www.newyorkjets.com/\", \n",
    "    # AFC North\n",
    "    \"https://www.baltimoreravens.com/\", \"https://www.bengals.com/\", \"https://www.clevelandbrowns.com/\", \"https://www.steelers.com/\",    \n",
    "    # AFC South\n",
    "    \"https://www.houstontexans.com/\", \"https://www.colts.com/\", \"https://www.jaguars.com/\", \"https://www.tennesseetitans.com/\",    \n",
    "    # AFC West\n",
    "    \"https://www.denverbroncos.com/\", \"https://www.chiefs.com/\", \"https://www.raiders.com/\", \"https://www.chargers.com/\",    \n",
    "    # NFC East\n",
    "    \"https://www.dallascowboys.com/\", \"https://www.giants.com/\", \"https://www.philadelphiaeagles.com/\", \"https://www.commanders.com/\",    \n",
    "    # NFC North\n",
    "    \"https://www.chicagobears.com/\", \"https://www.detroitlions.com/\", \"https://www.packers.com/\", \"https://www.vikings.com/\",    \n",
    "    # NFC South\n",
    "    \"https://www.atlantafalcons.com/\", \"https://www.panthers.com/\", \"https://www.neworleanssaints.com/\", \"https://www.buccaneers.com/\",    \n",
    "    # NFC West\n",
    "    \"https://www.azcardinals.com/\", \"https://www.therams.com/\", \"https://www.49ers.com/\", \"https://www.seahawks.com/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caacda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_team_abbreviations = [\n",
    "    # AFC East\n",
    "    \"BUF\", \"MIA\", \"NE\", \"NYJ\", \n",
    "    # AFC North\n",
    "    \"BAL\", \"CIN\", \"CLE\", \"PIT\",\n",
    "    # AFC South\n",
    "    \"HOU\", \"IND\", \"JAX\", \"TEN\", \n",
    "    # AFC West\n",
    "    \"DEN\", \"KC\", \"LV\", \"LAC\", \n",
    "    # NFC East\n",
    "    \"DAL\", \"NYG\", \"PHI\", \"WAS\",\n",
    "    # NFC North\n",
    "    \"CHI\", \"DET\", \"GB\", \"MIN\",\n",
    "    # NFC South\n",
    "    \"ATL\", \"CAR\", \"NO\", \"TB\",\n",
    "    # NFC West\n",
    "    \"ARI\", \"LAR\", \"SF\", \"SEA\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a242bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "\n",
    "def get_team_injury_report(game_week: str, team_site_url: str, nfl_team_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrapes a single NFL team's injury report for a specific week from a dynamically\n",
    "    loaded team website (based on the common NFL league platform template).\n",
    "\n",
    "    Args:\n",
    "        game_week (str): The specific week number/identifier (e.g., \"1\", \"10\", \"REG-10\").\n",
    "        team_site_url (str): The base URL of the team's injury report page \n",
    "                             (e.g., \"https://www.commanders.com/team/injury-report/week/\").\n",
    "        nfl_team_name (str): The team abbreviation (e.g., \"WAS\") used for error logging.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The successfully parsed injury report table, or an empty DataFrame \n",
    "                      (pd.DataFrame()) if scraping or parsing fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Initialize WebDriver üåê\n",
    "    try:\n",
    "        # Use webdriver.Chrome() which is generally reliable.\n",
    "        # Requires the Selenium package (v4.6.0+) to automatically manage the ChromeDriver executable.\n",
    "        driver = webdriver.Chrome() \n",
    "    except Exception as e:\n",
    "        # Critical failure: WebDriver could not be initialized (e.g., browser not found, path issue)\n",
    "        print(f\"CRITICAL ERROR: WebDriver initialization failed. Team: {nfl_team_name}. Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Construct the final URL for the specific week (e.g., .../week/REG-10)\n",
    "    full_url = f\"{team_site_url}{game_week}\"\n",
    "    driver.get(full_url)\n",
    "    \n",
    "    # 2. Define the Target Element Selector\n",
    "    # This XPath targets the table element itself, relying on a specific CSS class \n",
    "    # used by the standard NFL web platform for the horizontal-scrolling injury table.\n",
    "    TABLE_XPATH = '//div[@class=\"d3-o-table--horizontal-scroll nfl-o-injury-report\"]/table'\n",
    "    \n",
    "    # 3. Explicit Wait for Dynamic Content Loading ‚è≥\n",
    "    # This is essential because the table content is often loaded via JavaScript\n",
    "    # AFTER the initial page HTML is returned.\n",
    "    try:\n",
    "        # Wait up to 20 seconds for the element to appear in the DOM.\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            # Expected Condition: check if the table element is present\n",
    "            EC.presence_of_element_located((By.XPATH, TABLE_XPATH))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        # Failure: The table never appeared in the time limit (usually due to slow network or URL error)\n",
    "        print(f\"TIMEOUT: Table did not load within 20s. Team: {nfl_team_name}, Week: {game_week}.\")\n",
    "        driver.quit()\n",
    "        return pd.DataFrame()\n",
    "    except NoSuchElementException:\n",
    "        # Failure: The XPath structure for the table is incorrect or the team uses a custom site.\n",
    "        print(f\"XPATH ERROR: Table element not found. Team: {nfl_team_name}, Week: {game_week}. (Likely a custom site).\")\n",
    "        driver.quit()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 4. Extract HTML and Parse with Pandas üìÑ\n",
    "    try:\n",
    "        # Get the complete HTML source of the located table element\n",
    "        table_element = driver.find_element(By.XPATH, TABLE_XPATH)\n",
    "        table_html = table_element.get_attribute('outerHTML')\n",
    "        \n",
    "        # Resolve Deprecation Warning: Wrap the HTML string in StringIO.\n",
    "        # This converts the string into a file-like object, which is the \n",
    "        # preferred input format for pd.read_html in recent pandas versions.\n",
    "        df_list = pd.read_html(StringIO(table_html))\n",
    "\n",
    "        if df_list:\n",
    "            # pd.read_html returns a list of DataFrames; we assume the first one is our target.\n",
    "            df = df_list[0]\n",
    "            \n",
    "            # Column Cleaning: Flatten any MultiIndex headers (common in scraped tables).\n",
    "            # If a column is a tuple (e.g., ('Player', 'Player')), it uses the first element.\n",
    "            df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]\n",
    "            \n",
    "            # --- FILE SAVING (Assumes 'team_filepath' is defined globally or passed) ---\n",
    "            # NOTE: This saving logic should ideally be handled outside this function \n",
    "            # to maintain separation of concerns (scraping vs. saving).\n",
    "            # It relies on the external variable 'team_filepath'.\n",
    "            # injury_filepath = team_filepath + nfl_team_name + game_week + '.csv'\n",
    "            # df.to_csv(injury_filepath)\n",
    "            \n",
    "            driver.quit()\n",
    "            return df\n",
    "        \n",
    "        # Failure: The HTML was found, but pandas could not parse a valid table structure from it.\n",
    "        print(f\"PARSE ERROR: Pandas found HTML but no table structure. Team: {nfl_team_name}, Week: {game_week}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        # General catch for any unexpected errors (e.g., Network issues, Pandas errors)\n",
    "        print(f\"UNEXPECTED ERROR: Failed to process table data. Team: {nfl_team_name}, Week: {game_week}. Exception: {e}\")\n",
    "        driver.quit()\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1beb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "# NOTE: You'll need other imports like 'os', 'time', 'By', etc., \n",
    "# for a fully functional version of this script.\n",
    "\n",
    "# --- DEPENDENCIES: Ensure these variables are defined elsewhere ---\n",
    "# nfl_team_urls: List of 32 official team website URLs\n",
    "# nfl_team_abbreviations: List of 32 official team abbreviations (e.g., 'BUF')\n",
    "# week_number: Integer representing the last week to scrape (e.g., 18)\n",
    "# season: String representing the current season (e.g., '2025')\n",
    "# get_team_injury_report: External function that handles the scraping (Selenium/BS4)\n",
    "\n",
    "def run_team_injury_reports():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the scraping of NFL injury reports for \n",
    "    all 32 teams across a specified range of regular season weeks.\n",
    "    \n",
    "    The function iterates through teams and weeks, calls an external function\n",
    "    to scrape the data, adds metadata, and consolidates all results into one file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the headers for the metadata columns to be added to the scraped data.\n",
    "    # These columns help uniquely identify the source of the injury report row.\n",
    "    inj_headers = ['season', 'game_type', 'team', 'week']\n",
    "    \n",
    "    # Initialize an empty master DataFrame with the metadata columns pre-defined.\n",
    "    inj_report_2025 = pd.DataFrame(columns=inj_headers)\n",
    "\n",
    "    # OUTER LOOP: Iterate through all 32 team URLs and their abbreviations simultaneously.\n",
    "    # The abbreviation (nfl_team_name) is used as metadata.\n",
    "    for team_url, nfl_team_name in zip(nfl_team_urls, nfl_team_abbreviations):\n",
    "        \n",
    "        i = 0\n",
    "        # INNER LOOP: Iterate through the specified number of weeks (from 1 up to week_number).\n",
    "        while i <= (week_number - 1):\n",
    "            i += 1\n",
    "            \n",
    "            # Construct the base URL for the injury report page.\n",
    "            # Example: https://www.commanders.com/team/injury-report/week/REG-\n",
    "            team_url1 = team_url + 'team/injury-report/week/REG-'\n",
    "            \n",
    "            # CALL EXTERNAL SCRAPING FUNCTION\n",
    "            # Calls the function to navigate the site, extract the table, and return a DataFrame.\n",
    "            # game_week is passed as '1', '2', '3', etc.\n",
    "            team_inj_df = get_team_injury_report(\n",
    "                game_week=str(i), \n",
    "                team_site_url=team_url1, \n",
    "                nfl_team_name=nfl_team_name\n",
    "            )\n",
    "\n",
    "            # --- Data Validation and Processing ---\n",
    "            \n",
    "            # Check if the scraping function failed or returned an empty table.\n",
    "            if team_inj_df.empty:\n",
    "                # Print a message indicating which URL/week failed.\n",
    "                print(f\"FAILED to scrape: {team_url}. Last successful week check: {i-1}\")\n",
    "\n",
    "            else:\n",
    "                # Prepare the constant values for the new metadata columns.\n",
    "                inj_head_values = [season, 'REG', nfl_team_name, i]\n",
    "                \n",
    "                # Assign Constant Metadata Columns:\n",
    "                # Loop through the headers and values, assigning the single value\n",
    "                # to the entire new column in the scraped DataFrame.\n",
    "                for header, head_value in zip(inj_headers, inj_head_values):\n",
    "                    team_inj_df[header] = head_value\n",
    "                \n",
    "                # Consolidate Data: Append the current team/week DataFrame \n",
    "                # to the master consolidation DataFrame.\n",
    "                # ignore_index=True re-calculates the index for the master DataFrame.\n",
    "                inj_report_2025 = pd.concat([inj_report_2025, team_inj_df], ignore_index=True)\n",
    "\n",
    "    # --- FINAL OUTPUT ---\n",
    "    \n",
    "    # Define the full file path for the consolidated output CSV.\n",
    "    # Note: Ensure the parent directories exist before running this section!\n",
    "    all_team_inj_report_filepath = 'C:/Users/sarae/Documents/NFL_Modeling/2025/Week11/all_team_inj_report.csv'\n",
    "\n",
    "    # Save the consolidated DataFrame to a CSV file.\n",
    "    # index=False prevents writing the pandas DataFrame row index to the CSV.\n",
    "    inj_report_2025.to_csv(all_team_inj_report_filepath, index=False) \n",
    "\n",
    "    # Confirmation message\n",
    "    print(f\"Successfully finished scraping. Data saved to: {all_team_inj_report_filepath}\")\n",
    "    \n",
    "    # Optional: Return the final DataFrame for further in-script use.\n",
    "    return inj_report_2025\n",
    "\n",
    "# Example function call (currently commented out)\n",
    "# injury_df = run_team_injury_reports()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
